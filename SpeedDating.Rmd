---
title: "Speed Dating"
author: "Josh Weber, Jared Lee, Michael Derkowski"
date: "2023-04-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(car)

```

## Data Mutations

```{r cars}
speed_dating = read.csv("speeddating.csv", header = T)

# Drop unnecessary columns
speed_dating = subset(speed_dating, select = -c(d_d_age, d_importance_same_race, d_importance_same_religion,
d_pref_o_attractive,d_pref_o_sincere,d_pref_o_intelligence,d_pref_o_funny,d_pref_o_ambitious,d_pref_o_shared_interests,
d_attractive_o,d_sinsere_o,d_intelligence_o,d_funny_o,d_ambitous_o,d_shared_interests_o,
d_attractive_important,d_sincere_important,d_intellicence_important,d_funny_important,d_ambtition_important,d_shared_interests_important,
d_attractive,d_sincere,d_intelligence,d_funny,d_ambition,
d_attractive_partner,d_sincere_partner,d_intelligence_partner,d_funny_partner,d_ambition_partner,d_shared_interests_partner,
d_sports,d_tvsports,d_exercise,d_dining,d_museums,d_art,d_hiking,d_gaming,d_clubbing,d_reading,d_tv,d_theater,d_movies,d_concerts,d_music,d_shopping,d_yoga,
d_interests_correlate,d_expected_happy_with_sd_people,d_expected_num_interested_in_me,d_expected_num_matches,d_guess_prob_liked,d_like,has_null
))

```





```{r}
# Convert b'1' to 1
speed_dating = speed_dating %>% mutate_at(vars(decision, decision_o, match, gender, race, race_o, samerace, field, ), function(x) gsub("^b'|'$", "", x))

speed_dating$decision = as.integer(speed_dating$decision)
speed_dating$decision_o = as.integer(speed_dating$decision_o)
speed_dating$match = as.factor(as.integer(speed_dating$match))
speed_dating$samerace = as.factor(speed_dating$samerace)
speed_dating$race = as.factor(speed_dating$race)
speed_dating$race_o = as.factor(speed_dating$race_o)
speed_dating$field = as.factor(speed_dating$field)
speed_dating$gender = as.factor(speed_dating$gender)


```



```{r}
# Fill in NA values with median for column
speed_dating = speed_dating %>% mutate(across(c(age, age_o, importance_same_religion, pref_o_attractive, pref_o_sincere, pref_o_intelligence, pref_o_funny, pref_o_ambitious, pref_o_shared_interests, attractive_o, sinsere_o, intelligence_o, funny_o, ambitous_o, shared_interests_o, attractive_important, sincere_important, guess_prob_liked, intellicence_important, funny_important, ambtition_important, shared_interests_important, attractive, sincere, intelligence, funny, ambition, attractive_partner, sincere_partner, intelligence_partner, funny_partner, importance_same_race, ambition_partner,  shared_interests_partner, sports, tvsports, exercise, dining, museums, art, hiking, gaming, clubbing, reading, tv, theater,  movies, concerts, music, shopping, yoga, interests_correlate, expected_happy_with_sd_people, expected_num_interested_in_me, expected_num_matches, like, met), ~replace_na(., median(., na.rm=TRUE))))



# Add overall criteria met column
speed_dating = speed_dating %>% mutate(
  criteria_o_percent = (pref_o_attractive * attractive_o 
                        + pref_o_sincere * sinsere_o 
                        + pref_o_intelligence * intelligence_o 
                        + pref_o_funny * funny_o 
                        + pref_o_ambitious * ambitous_o 
                        + pref_o_shared_interests * shared_interests_o) / 1000,
  criteria_percent = (attractive_important * attractive_partner 
                      + sincere_important * sincere_partner 
                      + intellicence_important * intelligence_partner 
                      + funny_important * funny_partner 
                      + ambtition_important * ambition_partner 
                      + shared_interests_important * shared_interests_partner) / 1000
)

# Male only frame
male <- speed_dating %>%
  filter(gender == "male")


# Female only frame
female <- speed_dating %>%
  filter(gender == "female")

```


## Match Percent

```{r}
sum(speed_dating$match == 1) / dim(speed_dating)[1]
```



## Male vs. Female Important Qualities

```{r}


combined_pref = data.frame(
  preference = rep(c("attractive", "sincere", "intelligence", "funny", "ambition", "shared interests"), 2),
  average = c(
    mean(na.omit(male$attractive_important)),
    mean(na.omit(male$sincere_important)), 
    mean(na.omit(male$intellicence_important)), 
    mean(na.omit(male$funny_important)), 
    mean(na.omit(male$ambtition_important)), 
    mean(na.omit(male$shared_interests_important)),
    mean(na.omit(female$attractive_important)),
    mean(na.omit(female$sincere_important)), 
    mean(na.omit(female$intellicence_important)), 
    mean(na.omit(female$funny_important)), 
    mean(na.omit(female$ambtition_important)), 
    mean(na.omit(female$shared_interests_important))
    ),
  gender = c(rep('male',6), rep('female', 6))
)

ggplot(combined_pref, aes(preference, average, fill = gender)) + 
  geom_col(position = "dodge") +
  xlab("Trait") + 
  ylab("Average %") +
  ggtitle("Average % for Each Trait")


```




## Variable Selection

```{r}
everything_model<-glm(match ~ wave + gender + age + age_o + d_age + race + race_o + 
    samerace + importance_same_race + importance_same_religion + 
    pref_o_attractive + pref_o_sincere + pref_o_intelligence + 
    pref_o_funny + pref_o_ambitious + pref_o_shared_interests + 
    attractive_o + sinsere_o + intelligence_o + funny_o + ambitous_o + 
    shared_interests_o + attractive_important + sincere_important + 
    intellicence_important + funny_important + ambtition_important + 
    shared_interests_important + attractive + sincere + intelligence + 
    funny + ambition + attractive_partner + sincere_partner + 
    intelligence_partner + funny_partner + ambition_partner + 
    shared_interests_partner + sports + tvsports + exercise + 
    dining + museums + art + hiking + gaming + clubbing + reading + 
    tv + theater + movies + concerts + music + shopping + yoga + 
    interests_correlate + expected_happy_with_sd_people + expected_num_interested_in_me + 
    expected_num_matches + like + guess_prob_liked + met + criteria_o_percent + criteria_percent, family = "binomial", data=speed_dating)
stepAIC(everything_model)
```



## Create Training and testing set

```{r,comment=NA}
set.seed(1000)
index<- sample(dim(speed_dating)[1], round(dim(speed_dating)[1]*0.8))
train<- speed_dating[index,]
test<- speed_dating[-index,]
```



## Logistic Regression 

```{r, comment=NA}

mdl = glm(formula = match ~ d_age + race + race_o + importance_same_religion + 
    pref_o_attractive + pref_o_sincere + pref_o_ambitious + attractive_o + 
    sinsere_o + funny_o + ambitous_o + shared_interests_o + attractive_important + 
    sincere_important + intellicence_important + funny_important + 
    ambtition_important + shared_interests_important + attractive + 
    intelligence + attractive_partner + sincere_partner + intelligence_partner + 
    ambition_partner + tvsports + museums + art + clubbing + 
    reading + tv + movies + concerts + shopping + expected_num_interested_in_me + 
    expected_num_matches + like + guess_prob_liked + criteria_o_percent + 
    criteria_percent, family = "binomial", data = speed_dating)




summary(mdl)$coef

vif(mdl)

```

## After Removing Colinearity

```{r}
mdl =glm(formula = match ~ d_age + race + race_o + importance_same_religion + 
    pref_o_attractive + pref_o_sincere + pref_o_ambitious + attractive_o + 
    sinsere_o + funny_o + ambitous_o + shared_interests_o + funny_important + 
    ambtition_important + shared_interests_important + attractive + 
    intelligence + attractive_partner + sincere_partner + intelligence_partner + 
    ambition_partner + tvsports + museums + art + clubbing + 
    reading + tv + movies + concerts + shopping + expected_num_interested_in_me + 
    expected_num_matches + like + guess_prob_liked, family = "binomial", data = speed_dating)




summary(mdl)$coef

vif(mdl)
```



## Prediction Accuracy Using Confusion matrix

```{r, comment=NA}
prob1<-stats::predict(mdl, newdata=test)#  this will result in log odds ratio
prob1<-stats::predict(mdl, newdata=test, type="response") 
# `type=response` provides predicted probability 


pred1<- rep(0,dim(test)[1]) ## create a zero vector, there are 115 rows  in the training set

pred1[prob1>0.4]=1 ## if predicted probability is more than 60%, we mark that as a survived.
tab1<-table(pred1, test$match) # confusion matrix
addmargins(tab1)
 sum(diag(tab1))/sum(tab1)# 

```



## Visualize the Logistic Model

```{r, comment=NA}

params = c('d_age', 'importance_same_religion', 'pref_o_attractive', 'pref_o_sincere', 'pref_o_ambitious', 'attractive_o', 'sinsere_o', 'funny_o', 'ambitous_o', 'shared_interests_o', 'funny_important', 'ambtition_important', 'shared_interests_important', 'attractive_partner', 'sincere_partner', 'intelligence_partner', 'ambition_partner', 'tvsports', 'museums', 'art', 'clubbing', 'reading', 'tv', 'movies', 'concerts', 'shopping', 'expected_num_interested_in_me', 'expected_num_matches', 'like', 'guess_prob_liked'
)

for(param in params){
  data=broom::augment(mdl,type.predict = "response")
  print(ggplot(data, aes(x= .data[[param]]))+
  #geom_jitter(aes(y=Survived))+
    geom_smooth( aes(y=.fitted)))
}



```


The goal of this research question was to create the best possible model to predict whether a match would occur without using the decisions from either individual. This proved to be very difficult due to the number of variables present and the nature of human interaction in relationships.

The initial idea was to create two columns `criteria_percent` and `criteria_o_percent` which represented weighted ratings of what each partner marked as important and what they rated the other. These values were calculated on a scale from 0 to 1. Although this seemed to be an interesting way to capture the overall thoughts and feelings about the other partner it proved to not be very useful. We believe this is because we of lose data when consolidating many columns into one. The other thing to keep in mind is what people think is important may not be what's actually affecting the results and by consolidating the column the model loses the ability to determine what is objectively most important.

To create this final model we first used the StepAIC function to and used the model with the lowest AIC value. From there we examined the model using VIF to determine parameters that were colinear. The colinear parameters pruned in this process were `attractive_important`, `sincere_important`, `intellicence_important`, `criteria_o_percent`, `criteria_percent`. This refers back to the earlier point about consolidating the columns and losing data, as the weighted criteria columns had colinearity with other columns since they were derived from them.

Out of all interactions and models tested ,this model does have the best accuracy percentage of 87.4% with a 80/20 split of train and test data. This seems incredibly accurate however it is not that impressive. Since the dataset contains only ~16% matches the model could predict all non-matches and be 84% right. This means that the model has trouble being able to accurately predict matches which can be seen by the 129 false positives and 121 true positives, leaving the accuracy of matches less than 50%. Due to the nature of human interaction and relationships more coplex data and analysis may need to be done to improve the model. Additionally, it's completely reasonable that a nearly perfect match on paper could have a horrible first interaction meaning that the match doesn't happen. This specific example is one of many reasons why it could be extremely difficult to predict matches with this dataset.

Lastly, we have provided visualations of the logistic model with nearly all of the parameters. Many of these graphs show interesting patterns about the parameter's affect on the chance of a match. For example we can see that the statistics collected for the `like` meaning how much you liked your partner and `guess_prop_liked` meaning how much you think your partner liked you appear to have a strong affect on the models prediction. Additionally, The variables `amibtion_important` and `pref_o_ambitious` appear to have almost no effect on the model due to their straight line visualizations. This may mean they can be pruned from the model, however more tweaking and analysis of the model may need to be performed because removing these does decrease the accuracy in our tests.













